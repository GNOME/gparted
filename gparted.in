#!/bin/sh
#     Name:  gparted
#  Purpose:  Perform appropriate startup of GParted executable gpartedbin.
#
#            The purpose of these startup methods is to prevent
#            devices from being automounted, and to ensure only one
#            instance of GParted is running.  File system problems can
#            occur if devices are mounted prior to the completion of
#            GParted's operations, or if multiple partition editing
#            tools are in use concurrently.
#
# Copyright (C) 2008, 2009, 2010, 2013, 2015 Curtis Gedak
#
#  This file is part of GParted.
#
#  GParted is free software; you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation; either version 2 of the License, or
#  (at your option) any later version.
#
#  GParted is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with GParted.  If not, see <http://www.gnu.org/licenses/>.
#

#
#  Only permit one instance of GParted to execute at a time
#
if pidof gpartedbin 1> /dev/null; then
	echo "The process gpartedbin is already running."
	echo "Only one gpartedbin process is permitted."
	exit 1
fi

#
#  Define base command for executing GParted
#
BASE_CMD="@libexecdir@/gpartedbin $*"

#
#  For non-root users try to get authorisation to run GParted as root.
#
if test "x`id -u`" != "x0"; then
	#
	#  If there is no configured SU program run gpartedbin as
	#  non-root to display the graphical error about needing root
	#  privileges.
	#
	if test "x@gksuprog@" = "x"; then
		echo "Root privileges are required for running gparted."
		$BASE_CMD
		exit 1
	fi

	#
	# Interim workaround to allow GParted run by root access to the
	# X11 display server under Wayland.  If configured with
	# './configure --enable-xhost-root', the xhost command is
	# available and root has not been granted access to the X11
	# display via xhost, then grant access.
	#
	ENABLE_XHOST_ROOT=@enable_xhost_root@
	GRANTED_XHOST_ROOT=no
	if test "x$ENABLE_XHOST_ROOT" = 'xyes' && xhost 1> /dev/null 2>&1; then
		if ! xhost | grep -qi 'SI:localuser:root$'; then
			xhost +SI:localuser:root
			GRANTED_XHOST_ROOT=yes
		fi
	fi

	#
	# Run gparted as root.
	#
	@gksuprog@ '@bindir@/gparted' "$@"
	status=$?

	#
	# Revoke root access to the X11 display, only if we granted it.
	#
	if test "x$GRANTED_XHOST_ROOT" = 'xyes'; then
		xhost -SI:localuser:root
	fi
	exit $status
fi

#
#  Search PATH to determine if systemctl program can be found
#  and if appropriate daemon is running.
#
HAVE_SYSTEMCTL=no
for k in '' `echo "$PATH" | sed 's,:, ,g'`; do
	if test -x "$k/systemctl"; then
		if pidof systemd 1> /dev/null; then
			HAVE_SYSTEMCTL=yes
			break
		fi
	fi
done

#
#  Check if udisks2-inhibit exists in a known location
#  and if appropriate daemon is running.
#
HAVE_UDISKS2_INHIBIT=no
for k in /usr/libexec/udisks2/udisks2-inhibit \
         /usr/lib/udisks2/udisks2-inhibit; do
	if test -x $k; then
		if pidof udisksd 1> /dev/null; then
			HAVE_UDISKS2_INHIBIT=yes
			UDISKS2_INHIBIT_BIN=$k
			break
		fi
	fi
done

#
#  Search PATH to determine if udisks program can be found
#  and if appropriate daemon is running.
#
HAVE_UDISKS=no
for k in '' `echo "$PATH" | sed 's,:, ,g'`; do
	if test -x "$k/udisks"; then
		if pidof udisks-daemon 1> /dev/null; then
			HAVE_UDISKS=yes
			break
		fi
	fi
done

#
#  Search PATH to determine if hal-lock program can be found
#  and if appropriate daemon is running.
#
HAVE_HAL_LOCK=no
for k in '' `echo "$PATH" | sed 's,:, ,g'`; do
	if test -x "$k/hal-lock"; then
		if pidof hald 1> /dev/null; then
			HAVE_HAL_LOCK=yes
			break
		fi
	fi
done

#
#  Use systemctl to prevent automount by masking currently unmasked mount points
#
MOUNTLIST=''
if test "x$HAVE_SYSTEMCTL" = "xyes"; then
	MOUNTLIST=`systemctl show --all --property=Where,What,Id,LoadState '*.mount' | \
	awk '
	function clear_properties() {
		where = ""; what = ""; id = ""; loadstate = ""
	}
	function process_unit() {
		if (substr(what,1,5) == "/dev/"     &&
		    loadstate        != "masked"    &&
		    what             != "/dev/fuse" &&
		    where            != "/"         &&
		    ! (substr(what,1,9) == "/dev/loop" && substr(where,1,6) == "/snap/"))
		{
			print id
		}
		clear_properties()
	}
	/^Where=/     { where     = substr($0,7) }
	/^What=/      { what      = substr($0,6) }
	/^Id=/        { id        = substr($0,4) }
	/^LoadState=/ { loadstate = substr($0,11) }
	/^$/          { process_unit() }
	END           { process_unit() }
	'`
	if test "x$MOUNTLIST" != "x"; then
		systemctl --runtime mask --quiet -- $MOUNTLIST
	fi
fi

#
# Create temporary blank overrides for all udev rules which automatically
# start Linux Software RAID array members, LVM Volume Groups and Bcache devices.
#
# Udev stores volatile / temporary runtime rules in directory /run/udev/rules.d.
# Volatile / temporary rules are used to override system default rules from
# /lib/udev/rules.d and/or /usr/lib/udev/rules.d (depending on udev
# configuration).  (Permanent local administrative rules in directory
# /etc/udev/rules.d override all others).  See udev(7) manual page for details.
#
# Default udev rules containing mdadm to incrementally start array members are
# found in 64-md-raid.rules and/or 65-md-incremental.rules, depending on the
# distribution and age.  The rules may be commented out or not exist at all.
#
UDEV_TEMP_RULES=''  # List of temporary override rules files.
if test -d /run/udev; then
	test ! -d /run/udev/rules.d && mkdir /run/udev/rules.d
	UDEV_TEMP_RULES=`for udev_default_rules_dir in /lib/udev/rules.d /usr/lib/udev/rules.d
	do
		test -d $udev_default_rules_dir || continue
		egrep -l '^[^#].*mdadm (-I|--incremental)' $udev_default_rules_dir/*.rules 2> /dev/null
		egrep -l 'lvm vgchange -a' $udev_default_rules_dir/*lvm*.rules 2> /dev/null
		ls $udev_default_rules_dir/*bcache*.rules 2> /dev/null
	done | sed 's,.*/lib/udev,/run/udev,g' | sort -u`
fi
for rule in $UDEV_TEMP_RULES; do
	touch "$rule"
done

#
#  Use udisks2-inhibit if udisks2-inhibit exists and deamon running.
#  Else use both udisks and hal-lock for invocation if both binaries exist and both
#  daemons are running.
#  Else use udisks if binary exists and daemon is running.
#  Otherwise use hal-lock for invocation if binary exists and daemon is running.
#  If the above checks fail then simply run gpartedbin.
#
if test "x$HAVE_UDISKS2_INHIBIT" = "xyes"; then
	$UDISKS2_INHIBIT_BIN $BASE_CMD
elif test "x$HAVE_UDISKS" = "xyes" && test "x$HAVE_HAL_LOCK" = "xyes"; then
	udisks --inhibit -- \
		hal-lock --interface org.freedesktop.Hal.Device.Storage --exclusive \
			--run "$BASE_CMD"
elif test "x$HAVE_UDISKS" = "xyes"; then
	udisks --inhibit -- $BASE_CMD
elif test "x$HAVE_HAL_LOCK" = "xyes"; then
	hal-lock --interface org.freedesktop.Hal.Device.Storage --exclusive \
		--run "$BASE_CMD"
else
	$BASE_CMD
fi
status=$?

#
# Clear any temporary override udev rules used to stop udev automatically
# starting Linux Software RAID array members, LVM Volume Groups and Bcache
# devices.
#
for rule in $UDEV_TEMP_RULES; do
	rm -f "$rule"
done

#
#  Use systemctl to unmask those mount points masked above
#
if test "x$HAVE_SYSTEMCTL" = "xyes" && test "x$MOUNTLIST" != "x"; then
	systemctl --runtime unmask --quiet -- $MOUNTLIST 2> /dev/null || \
	{
		cd /run/systemd/system  &&
		rm -f -- $MOUNTLIST     &&
		systemctl daemon-reload
	}
fi

exit $status
